{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Regression Analysis\n",
    "### by Charlie LaBarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.sql.functions import col, avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = spark.read.csv(\"train.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic description of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- MSSubClass: string (nullable = true)\n",
      " |-- MSZoning: string (nullable = true)\n",
      " |-- LotFrontage: string (nullable = true)\n",
      " |-- LotArea: string (nullable = true)\n",
      " |-- Street: string (nullable = true)\n",
      " |-- Alley: string (nullable = true)\n",
      " |-- LotShape: string (nullable = true)\n",
      " |-- LandContour: string (nullable = true)\n",
      " |-- Utilities: string (nullable = true)\n",
      " |-- LotConfig: string (nullable = true)\n",
      " |-- LandSlope: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Condition1: string (nullable = true)\n",
      " |-- Condition2: string (nullable = true)\n",
      " |-- BldgType: string (nullable = true)\n",
      " |-- HouseStyle: string (nullable = true)\n",
      " |-- OverallQual: string (nullable = true)\n",
      " |-- OverallCond: string (nullable = true)\n",
      " |-- YearBuilt: string (nullable = true)\n",
      " |-- YearRemodAdd: string (nullable = true)\n",
      " |-- RoofStyle: string (nullable = true)\n",
      " |-- RoofMatl: string (nullable = true)\n",
      " |-- Exterior1st: string (nullable = true)\n",
      " |-- Exterior2nd: string (nullable = true)\n",
      " |-- MasVnrType: string (nullable = true)\n",
      " |-- MasVnrArea: string (nullable = true)\n",
      " |-- ExterQual: string (nullable = true)\n",
      " |-- ExterCond: string (nullable = true)\n",
      " |-- Foundation: string (nullable = true)\n",
      " |-- BsmtQual: string (nullable = true)\n",
      " |-- BsmtCond: string (nullable = true)\n",
      " |-- BsmtExposure: string (nullable = true)\n",
      " |-- BsmtFinType1: string (nullable = true)\n",
      " |-- BsmtFinSF1: string (nullable = true)\n",
      " |-- BsmtFinType2: string (nullable = true)\n",
      " |-- BsmtFinSF2: string (nullable = true)\n",
      " |-- BsmtUnfSF: string (nullable = true)\n",
      " |-- TotalBsmtSF: string (nullable = true)\n",
      " |-- Heating: string (nullable = true)\n",
      " |-- HeatingQC: string (nullable = true)\n",
      " |-- CentralAir: string (nullable = true)\n",
      " |-- Electrical: string (nullable = true)\n",
      " |-- 1stFlrSF: string (nullable = true)\n",
      " |-- 2ndFlrSF: string (nullable = true)\n",
      " |-- LowQualFinSF: string (nullable = true)\n",
      " |-- GrLivArea: string (nullable = true)\n",
      " |-- BsmtFullBath: string (nullable = true)\n",
      " |-- BsmtHalfBath: string (nullable = true)\n",
      " |-- FullBath: string (nullable = true)\n",
      " |-- HalfBath: string (nullable = true)\n",
      " |-- BedroomAbvGr: string (nullable = true)\n",
      " |-- KitchenAbvGr: string (nullable = true)\n",
      " |-- KitchenQual: string (nullable = true)\n",
      " |-- TotRmsAbvGrd: string (nullable = true)\n",
      " |-- Functional: string (nullable = true)\n",
      " |-- Fireplaces: string (nullable = true)\n",
      " |-- FireplaceQu: string (nullable = true)\n",
      " |-- GarageType: string (nullable = true)\n",
      " |-- GarageYrBlt: string (nullable = true)\n",
      " |-- GarageFinish: string (nullable = true)\n",
      " |-- GarageCars: string (nullable = true)\n",
      " |-- GarageArea: string (nullable = true)\n",
      " |-- GarageQual: string (nullable = true)\n",
      " |-- GarageCond: string (nullable = true)\n",
      " |-- PavedDrive: string (nullable = true)\n",
      " |-- WoodDeckSF: string (nullable = true)\n",
      " |-- OpenPorchSF: string (nullable = true)\n",
      " |-- EnclosedPorch: string (nullable = true)\n",
      " |-- 3SsnPorch: string (nullable = true)\n",
      " |-- ScreenPorch: string (nullable = true)\n",
      " |-- PoolArea: string (nullable = true)\n",
      " |-- PoolQC: string (nullable = true)\n",
      " |-- Fence: string (nullable = true)\n",
      " |-- MiscFeature: string (nullable = true)\n",
      " |-- MiscVal: string (nullable = true)\n",
      " |-- MoSold: string (nullable = true)\n",
      " |-- YrSold: string (nullable = true)\n",
      " |-- SaleType: string (nullable = true)\n",
      " |-- SaleCondition: string (nullable = true)\n",
      " |-- SalePrice: string (nullable = true)\n",
      "\n",
      "Training set length: 1460\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()\n",
    "print \"Training set length: \" + str(train_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature shaping + pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for feature shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a dataframe and a list of column names, cast those columns to double\n",
    "def convertColsToDbl(dataframe, col_names):\n",
    "    for col_name in col_names:\n",
    "        # convert the column and drop the old column\n",
    "        dataframe = dataframe.withColumn(col_name + \"_asDbl\", dataframe[col_name].cast(\"double\")).drop(col_name)\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStringColumns(dataframe):\n",
    "    strcols = []\n",
    "    for column in dataframe.columns:\n",
    "        # if its a string column, add to the list\n",
    "        if(str(dataframe.schema[column].dataType) == 'StringType'):\n",
    "            strcols.append(column)\n",
    "    \n",
    "    return strcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a dataframe, convert the given columns to indexed columns (for pushing into a onehotencoder)\n",
    "def stringIndexDf(dataframe, strcols):\n",
    "    for column in strcols:\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column+\"_index\")\n",
    "        dataframe = indexer.fit(dataframe).transform(dataframe)\n",
    "        dataframe = dataframe.drop(column)\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHotDf(dataframe, strcols):\n",
    "    for column in strcols:\n",
    "        encoder = OneHotEncoder(inputCol=column+\"_index\", outputCol=column+\"_onehot\", dropLast=False)\n",
    "        dataframe = encoder.transform(dataframe)\n",
    "        dataframe = dataframe.drop(column+\"_index\")\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to deal with null values\n",
    "def fillNullVals(dataframe):\n",
    "    for column in dataframe.columns:\n",
    "        # if string column, replace with \"Unknown\" string\n",
    "        if(str(dataframe.schema[column].dataType) == 'StringType'):\n",
    "            dataframe = dataframe.fillna(\"Unknown\", [column])\n",
    "        elif (str(dataframe.schema[column].dataType) == 'DoubleType'):\n",
    "            # calculate average value\n",
    "            nonnull = dataframe.dropna(subset=[column])\n",
    "            colAvg = nonnull.agg(avg(col(column)))\n",
    "            colAvg = colAvg.rdd.map(lambda x: x[0]).first()\n",
    "                        \n",
    "            # replace null values with average value\n",
    "            dataframe = dataframe.fillna(colAvg, [column])\n",
    "            \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare initial columns to convert to double val, and convert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doubleCols = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
    "              'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n",
    "              'GrLivArea', 'GarageYrBlt', 'GarageArea', 'MiscVal', 'YrSold',\n",
    "              'WoodDeckSf', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n",
    "              'MasVnrArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = convertColsToDbl(train_df, doubleCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = fillNullVals(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index and one-hot encode string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the ids to a different df in case we need them, then drop them\n",
    "ids_df = train_df.select(\"Id\")\n",
    "train_df = train_df.drop(\"Id\")\n",
    "\n",
    "# save the price column as the label\n",
    "train_df = train_df.withColumn(\"label\", train_df[\"SalePrice\"].cast(\"Double\")).drop(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract list of string columns\n",
    "string_cols = getStringColumns(train_df)\n",
    "\n",
    "# index the string columns, then one hot encode the indexed string columns \n",
    "train_df = stringIndexDf(train_df, string_cols)\n",
    "train_df = oneHotDf(train_df, string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove label column from columns to be assembled\n",
    "columns_less_label = list(train_df.columns)\n",
    "columns_less_label.remove('label')\n",
    "\n",
    "feature_pipeline = Pipeline(stages= [VectorAssembler(inputCols=columns_less_label, outputCol=\"features\")])\n",
    "\n",
    "train_df_transformed = feature_pipeline.fit(train_df).transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(maxDepth=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|   label|prediction|\n",
      "+--------+----------+\n",
      "|208500.0|  213172.5|\n",
      "|181500.0|  176000.0|\n",
      "|223500.0|  223100.0|\n",
      "|140000.0| 143120.25|\n",
      "|250000.0|  267990.0|\n",
      "+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sanity check before doing actual cross-validation of model\n",
    "rf_fitted_on_all = rf_model.fit(train_df_transformed)\n",
    "rf_fitted_on_all.transform(train_df_transformed).select(\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-fold cross validation on RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().build()\n",
    "crossValidator = CrossValidator(estimator=rf_model,\n",
    "                                estimatorParamMaps=paramGrid,\n",
    "                                evaluator=RegressionEvaluator(),\n",
    "                                numFolds=3)\n",
    "\n",
    "rf_validated = crossValidator.fit(train_df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30774.58762688168]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_validated.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Different Feature Shaping Strategies and Impact on Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer, MinMaxScaler, PCA\n",
    "from pyspark.sql.functions import min, max, udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: getting training data back in unshaped form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = spark.read.csv(\"train.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = convertColsToDbl(train_df, doubleCols)\n",
    "train_df = fillNullVals(train_df)\n",
    "\n",
    "# save the ids to a different df in case we need them, then drop them\n",
    "ids_df = train_df.select(\"Id\")\n",
    "train_df = train_df.drop(\"Id\")\n",
    "\n",
    "# save the price column as the label\n",
    "train_df = train_df.withColumn(\"label\", train_df[\"SalePrice\"].cast(\"Double\")).drop(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for feature transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to generate bin columns for a list of features\n",
    "def binFeatures(dataframe, columnsToBin, numBins=10):\n",
    "    for column in columnsToBin:\n",
    "        minVal = dataframe.agg(min(col(column))).rdd.map(lambda x: x[0]).first()\n",
    "        maxVal = dataframe.agg(max(col(column))).rdd.map(lambda x: x[0]).first()\n",
    "        \n",
    "        rangeVal = maxVal - minVal\n",
    "        \n",
    "        splits = [-float(\"inf\")]\n",
    "        \n",
    "        bucketSize = 1.0*rangeVal/numBins\n",
    "        \n",
    "        for i in range(numBins):\n",
    "            splits.append(float((minVal + i*bucketSize)))\n",
    "            \n",
    "        splits.append(float(\"inf\"))\n",
    "                    \n",
    "        bucketizer = Bucketizer(splits=splits,\n",
    "                                inputCol=column, outputCol=column+\"_bucketed\")\n",
    "        dataframe = bucketizer.transform(dataframe)\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeFeatures(dataframe, featuresToNormalize):\n",
    "    for column in featuresToNormalize:\n",
    "        va = VectorAssembler(inputCols=[column],\n",
    "                             outputCol=column+\"_vec\")\n",
    "        dataframe = va.transform(dataframe)\n",
    "        normalizer = MinMaxScaler(inputCol=column+\"_vec\", outputCol=column+\"_normed\")\n",
    "        dataframe = normalizer.fit(dataframe).transform(dataframe)\n",
    "        \n",
    "        dataframe = dataframe.drop(column).withColumn(column, dataframe[column+\"_normed\"]).drop(column+\"_vec\")\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addPCACols(dataframe, numDimensions=5):\n",
    "    pca = PCA(k=numDimensions, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "    va = VectorAssembler(inputCols=[\"features\", \"pcaFeatures\"],\n",
    "                             outputCol=\"features_final\")\n",
    "    \n",
    "    dataframe = pca.fit(dataframe).transform(dataframe)\n",
    "    dataframe = va.fit(dataframe).transform(dataframe)\n",
    "    dataframe = dataframe.drop(\"features\").withColumn(\"features\", dataframe[\"features_final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df_saved = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df = train_df_saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping of categoricals + Combinations of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: grouping of categoricals\n",
    "residentialCategories = ['RH', 'RL', 'RP', 'RM']\n",
    "zoningGrouping = udf(lambda x: 'Residential' if x in residentialCategories else 'Non-Residential')\n",
    "\n",
    "oneFloorCategories = ['20','30', '40']\n",
    "floorGrouping = udf(lambda x: 'OneFloor' if x in oneFloorCategories else 'NonOneFloor')\n",
    "\n",
    "train_df = train_df.withColumn(\"ResidentialOrNot\", zoningGrouping(train_df[\"MSZoning\"]))\n",
    "train_df = train_df.withColumn(\"OneFloor\", floorGrouping(train_df[\"MSZoning\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: combining variables\n",
    "train_df = train_df.withColumn(\"QualityCombined\", train_df[\"OverallQual_asDbl\"]*train_df[\"OverallCond_asDbl\"])\n",
    "train_df = train_df.withColumn(\"YearAndMonthCombined\", train_df[\"YrSold_asDbl\"] + (train_df[\"MoSold\"].cast(\"Double\")/12.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doubleCols_new = []\n",
    "for colname in doubleCols:\n",
    "    doubleCols_new.append(colname+\"_asDbl\")\n",
    "\n",
    "# Step 3: bin features\n",
    "train_df = binFeatures(train_df, doubleCols_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 4: normalize features\n",
    "train_df = normalizeFeatures(train_df, doubleCols_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on final feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get final feature vector in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract list of string columns\n",
    "string_cols = getStringColumns(train_df)\n",
    "\n",
    "# index the string columns, then one hot encode the indexed string columns \n",
    "train_df = stringIndexDf(train_df, string_cols)\n",
    "train_df = oneHotDf(train_df, string_cols)\n",
    "\n",
    "# remove label column from columns to be assembled\n",
    "columns_less_label = list(train_df.columns)\n",
    "columns_less_label.remove('label')\n",
    "\n",
    "feature_pipeline = Pipeline(stages= [VectorAssembler(inputCols=columns_less_label, outputCol=\"features\")])\n",
    "\n",
    "train_df_transformed = feature_pipeline.fit(train_df).transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_transformed = addPCACols(train_df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun of RF model, now with cool feature transformations :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder().build()\n",
    "crossValidator = CrossValidator(estimator=rf_model,\n",
    "                                estimatorParamMaps=paramGrid,\n",
    "                                evaluator=RegressionEvaluator(),\n",
    "                                numFolds=3)\n",
    "\n",
    "rf_validated = crossValidator.fit(train_df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_validated.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py 2)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
